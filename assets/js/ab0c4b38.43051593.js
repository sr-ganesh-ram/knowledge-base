"use strict";(self.webpackChunkkb_src=self.webpackChunkkb_src||[]).push([[6686],{7630:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>d,contentTitle:()=>i,default:()=>c,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"AI-ML/Intro/GenAI-Vs-FM-Vs-LLM","title":"GenAI-Vs-FM-Vs-LLM","description":"The terms Generative AI, Foundation Model, and Large Language Model (LLM) are related but not interchangeable.  Think of them as nested sets\\\\n\\\\n| Feature          | Generative AI                      | Foundation Model                     | Large Language Model (LLM)         |\\\\n|-----------------|--------------------------------------|---------------------------------------|------------------------------------|\\\\n| Scope        | Broadest category                    | Subset of Generative AI              | Subset of Foundation Models         |\\\\n| Data Focus   | Varied (text, images, audio, etc.) | Multimodal or unimodal (large scale) | Primarily text                        |\\\\n| Training     | Varies widely                        | Massive datasets, often self-supervised | Massive text datasets, self-supervised|\\\\n| Adaptability | Varies widely                        | Highly adaptable for diverse tasks    | Highly adaptable for language tasks |\\\\n| Output       | New content in various formats       | New content, adapted for different uses| Text-based content                   |\\\\n| Examples*      | GANs, Diffusion Models, LLMs        | LLMs, Image generation models       | GPT-3, LaMDA, PaLM                 |\\\\n\\\\n\\\\nIn short:  All LLMs are Foundation Models, and all Foundation Models are Generative AI, but not all Generative AI are Foundation Models, and not all Foundation Models are LLMs.  The categories represent a hierarchy of increasing specificity.\\\\n","source":"@site/docs/AI-ML/1.Intro/GenAI-Vs-FM-Vs-LLM.md","sourceDirName":"AI-ML/1.Intro","slug":"/AI-ML/Intro/GenAI-Vs-FM-Vs-LLM","permalink":"/knowledge-base/docs/AI-ML/Intro/GenAI-Vs-FM-Vs-LLM","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/AI-ML/1.Intro/GenAI-Vs-FM-Vs-LLM.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Foundation Models","permalink":"/knowledge-base/docs/AI-ML/Intro/Foundation Models"},"next":{"title":"Generative AI","permalink":"/knowledge-base/docs/AI-ML/Intro/Generative AI"}}');var s=n(4848),o=n(8453);const r={},i=void 0,d={},l=[];function u(e){const a={p:"p",strong:"strong",...(0,o.R)(),...e.components};return(0,s.jsxs)(a.p,{children:["The terms Generative AI, Foundation Model, and Large Language Model (LLM) are related but not interchangeable.  Think of them as nested sets:\\n\\n* ",(0,s.jsx)(a.strong,{children:"Generative AI"})," is the broadest category.  It encompasses any AI system capable of generating new content, be it text, images, audio, video, code, or other data formats.  This generation is often based on learned patterns and distributions from training data.  Examples include GANs (Generative Adversarial Networks), diffusion models, and, importantly, LLMs.\\n\\n* ",(0,s.jsx)(a.strong,{children:"Foundation Models"}),' are a subset of Generative AI (and sometimes include non-generative models). They are large, powerful models trained on massive datasets, often using self-supervised learning.  Their key characteristic is their ability to be adapted or "fine-tuned" for a wide range of downstream tasks with relatively little additional training data.  This adaptability is a crucial difference from models trained specifically for a single task.  LLMs are a prime example of foundation models.  Other examples include models trained on images, videos, or other modalities that can be adapted for diverse applications.\\n\\n* ',(0,s.jsx)(a.strong,{children:"Large Language Models (LLMs)"})," are a specific type of Foundation Model focused on text and language. They are trained on vast amounts of text data and can perform various language-related tasks such as text generation, translation, summarization, question answering, and code generation.  LLMs are a subset of both Foundation Models and Generative AI.  Examples include GPT-3, LaMDA, and PaLM.\\n\\n\\nHere's a table summarizing the key differences:\\n\\n| Feature          | Generative AI                      | Foundation Model                     | Large Language Model (LLM)         |\\n|-----------------|--------------------------------------|---------------------------------------|------------------------------------|\\n| ",(0,s.jsx)(a.strong,{children:"Scope"}),"        | Broadest category                    | Subset of Generative AI              | Subset of Foundation Models         |\\n| ",(0,s.jsx)(a.strong,{children:"Data Focus"}),"   | Varied (text, images, audio, etc.) | Multimodal or unimodal (large scale) | Primarily text                        |\\n| ",(0,s.jsx)(a.strong,{children:"Training"}),"     | Varies widely                        | Massive datasets, often self-supervised | Massive text datasets, self-supervised|\\n| ",(0,s.jsx)(a.strong,{children:"Adaptability"})," | Varies widely                        | Highly adaptable for diverse tasks    | Highly adaptable for language tasks |\\n| ",(0,s.jsx)(a.strong,{children:"Output"}),"       | New content in various formats       | New content, adapted for different uses| Text-based content                   |\\n| ",(0,s.jsx)(a.strong,{children:"Examples"}),"      | GANs, Diffusion Models, LLMs        | LLMs, Image generation models       | GPT-3, LaMDA, PaLM                 |\\n\\n\\nIn short:  All LLMs are Foundation Models, and all Foundation Models are Generative AI, but not all Generative AI are Foundation Models, and not all Foundation Models are LLMs.  The categories represent a hierarchy of increasing specificity.\\n"]})}function c(e={}){const{wrapper:a}={...(0,o.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},8453:(e,a,n)=>{n.d(a,{R:()=>r,x:()=>i});var t=n(6540);const s={},o=t.createContext(s);function r(e){const a=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function i(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:a},e.children)}}}]);