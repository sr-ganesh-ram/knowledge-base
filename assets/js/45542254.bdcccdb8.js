"use strict";(self.webpackChunkkb_src=self.webpackChunkkb_src||[]).push([[6774],{6901:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"AI-ML/Intro/Foundation Models","title":"Foundation Models","description":"Foundation Models \ud83c\udfd7\ufe0f - In 5 Minutes","source":"@site/docs/AI-ML/1.Intro/Foundation Models.md","sourceDirName":"AI-ML/1.Intro","slug":"/AI-ML/Intro/Foundation Models","permalink":"/knowledge-base/docs/AI-ML/Intro/Foundation Models","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/AI-ML/1.Intro/Foundation Models.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Deep Learning","permalink":"/knowledge-base/docs/AI-ML/Intro/Deep Learning"},"next":{"title":"GenAI-Vs-FM-Vs-LLM","permalink":"/knowledge-base/docs/AI-ML/Intro/GenAI-Vs-FM-Vs-LLM"}}');var r=i(4848),t=i(8453);const a={},o=void 0,l={},d=[{value:"Foundation Models \ud83c\udfd7\ufe0f - In 5 Minutes",id:"foundation-models-\ufe0f---in-5-minutes",level:2},{value:"\ud83c\udfd7\ufe0f What",id:"\ufe0f-what",level:3},{value:"\ud83c\udfaf Why",id:"-why",level:3},{value:"\u2699\ufe0f Where Applied",id:"\ufe0f-where-applied",level:3},{value:"\ud83e\udde0 How it Works",id:"-how-it-works",level:3},{value:"\ud83d\udd04 Lifecycle",id:"-lifecycle",level:3},{value:"\ud83d\udcca Diagram",id:"-diagram",level:3},{value:"\ud83d\udd17 Related Items",id:"-related-items",level:3}];function c(e){const n={h2:"h2",h3:"h3",li:"li",mermaid:"mermaid",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"foundation-models-\ufe0f---in-5-minutes",children:"Foundation Models \ud83c\udfd7\ufe0f - In 5 Minutes"}),"\n",(0,r.jsx)(n.h3,{id:"\ufe0f-what",children:"\ud83c\udfd7\ufe0f What"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Large-Scale AI Models:"})," Foundation models are large AI models trained on a vast amount of unlabeled data, designed to be adaptable to a wide range of downstream tasks."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"General-Purpose Capabilities:"})," These models are not specialized for a specific task but are designed to learn broad representations of the world from diverse data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pre-training and Fine-tuning:"})," They are pre-trained on large datasets and then fine-tuned for specific downstream applications."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transfer Learning:"})," Foundation models enable transfer learning, which means that knowledge gained in pre-training can be transferred to various other tasks."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Versatility:"})," They are versatile enough to support tasks across various modalities, including text, images, audio, and more."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-why",children:"\ud83c\udfaf Why"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reduced Training Time:"})," They reduce the need to train models from scratch for each new task, significantly saving time and resources."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Improved Performance:"})," Foundation models often achieve better performance on downstream tasks compared to models trained from scratch."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lower Data Requirements:"})," They can perform well even with limited labeled data for specific tasks due to the broad pre-training."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Faster Prototyping:"})," Enables faster prototyping of new AI applications due to their broad capabilities."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Innovation:"})," Facilitates the development of innovative and versatile AI-powered solutions."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"\ufe0f-where-applied",children:"\u2699\ufe0f Where Applied"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Natural Language Processing (NLP):"})," Text generation, summarization, translation, and question-answering."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Computer Vision:"})," Image recognition, object detection, and image generation."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Speech Recognition:"})," Transcribing speech to text and generating audio."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robotics:"})," Enabling robots to understand and interact with their environment."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Drug Discovery:"})," Identifying potential drug candidates and predicting their properties."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-how-it-works",children:"\ud83e\udde0 How it Works"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pre-training Phase:"})," The model is trained on a massive dataset of unlabeled data using self-supervised learning techniques."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Representation Learning:"})," The model learns general representations of data during pre-training."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fine-tuning Phase:"})," The pre-trained model is adapted to a specific downstream task using labeled data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transfer of Knowledge:"})," Knowledge learned during pre-training is transferred to improve performance on the downstream task."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Adaptation:"})," The model adapts to the specific requirements of the downstream task during fine-tuning."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-lifecycle",children:"\ud83d\udd04 Lifecycle"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Collection:"})," Gather massive amounts of diverse and unlabeled data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pre-training:"})," Train the model on the collected data to learn general representations."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Task Selection:"})," Identify the downstream task the model should be adapted for."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fine-tuning:"})," Adapt the pre-trained model to the specific task using labeled data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Evaluation:"})," Evaluate performance on the downstream task."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Deployment:"})," Deploy the model in real-world applications."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"-diagram",children:"\ud83d\udcca Diagram"}),"\n",(0,r.jsx)(n.mermaid,{value:"graph LR\r\n    A[Data Collection] --\x3e B(Pre-training);\r\n    B --\x3e C(Task Selection);\r\n    C --\x3e D(Fine-tuning);\r\n    D --\x3e E(Evaluation);\r\n\tE --\x3e F(Deployment)\r\n    style A fill:#f9f,stroke:#333,stroke-width:2px\r\n    style F fill:#ccf,stroke:#333,stroke-width:2px"}),"\n",(0,r.jsx)(n.h3,{id:"-related-items",children:"\ud83d\udd17 Related Items"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Self-Supervised Learning:"})," Learning from unlabeled data."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transfer Learning:"})," Using knowledge learned in one task to improve performance on another task."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Large Language Models (LLMs):"})," A type of foundation model trained for text-related tasks."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision Transformers (ViTs):"})," A type of foundation model trained for image-related tasks."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-Modal Learning:"})," Training models on multiple modalities, such as text and images."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(6540);const r={},t=s.createContext(r);function a(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);